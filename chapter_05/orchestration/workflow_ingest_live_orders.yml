main:
    params: [event]
    steps:
        - init:
            assign:
                - project_id: ${sys.get_env("PROJECT_ID")}
                - bucket: ${sys.get_env("BUCKET")}
                - job_location: ${sys.get_env("REGION")}                
                - job_name: ch05-batch-ingestion-orders
                - batch_id: ${"workflow-ingest-live-orders-" + sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}
                - service_account: ${sys.get_env("SERVICE_ACCOUNT")}                   

        - streaming_ingestion_orders:
            call: googleapis.run.v1.namespaces.jobs.run
            args:
                name: ${"namespaces/" + project_id + "/jobs/" + job_name}
                location: ${job_location}
            result: run_execution

        - spark_streaming_orders_to_bronze:
            call: http.post
            args:
                url: ${"https://dataproc.googleapis.com/v1/projects/" + project_id +"/locations/"+ job_location + "/batches"}
                auth:
                    type: OAuth2
                    scopes: "https://www.googleapis.com/auth/cloud-platform"
                headers:
                    Content-Type: "application/json"
                query:
                    batchId: ${batch_id}
                body:
                    environmentConfig:
                        executionConfig:
                            serviceAccount: ${service_account}
                    pysparkBatch:
                        mainPythonFileUri: ${"gs://" + bucket + "/chapter_05/ingestion/spark/jobs/streaming/01_lz_orders_to_bronze_layer.py"}
                        args:
                        - "--ttl=10m"
                        - ${"--GCS_BUCKET=" + bucket}
                        - ${"--INPUT_PATH=gs://" + bucket + "/landing_zone/streaming/orders/"}
                        - ${"--CHECKPOINT_PATH=gs://" + bucket + "/_checkpoints/ch05/orders_to_iceberg/"}
                        - ${"--QUARANTINE_PATH=gs://" + bucket + "/quarantine/live_orders/"}
                        - "--CATALOG_NAME=local"
                        - "--DB_NAME=bronze"
                        - "--TABLE_NAME=live_orders"
                        - "--MAX_FILES_PER_TRIGGER=50"
                        - "--TRIGGER_SECONDS=15"
                        - "--SPARK_LOG_LEVEL=WARN"
                    runtimeConfig:
                        version: "2.1"
                        properties:                         
                            spark.executor.instances: "2"
                            spark.driver.cores: "4"
                            spark.executor.cores: "4"
                            spark.dataproc.driver.disk.size: "250g"
                            spark.dataproc.executor.disk.size: "250g"
                            spark.jars.packages: "org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.10.1"
                            spark.sql.extensions: "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions"
                            spark.sql.catalog.local: "org.apache.iceberg.spark.SparkCatalog"
                            spark.sql.catalog.local.type: "hadoop"
                            spark.sql.catalog.local.warehouse: ${"gs://" + bucket + "/iceberg/warehouse"}
            result: dataproc_execution

        - check_job:
            call: http.get
            args:
                url: ${"https://dataproc.googleapis.com/v1/projects/"+ project_id +"/locations/"+ job_location +"/batches/" + batch_id}
                auth:
                    type: OAuth2
            result: jobStatus

        - check_job_done:
                switch:
                    - condition: ${jobStatus.body.state == "SUCCEEDED"}
                      next: finish
                    - condition: ${jobStatus.body.state == "FAILED" OR jobStatus.body.state == "CANCELLED" OR jobStatus.body.state == "UNSPECIFIED"}
                      raise: ${"Dataproc batch failed with state " + jobStatus.body.state}

        - wait:
            call: sys.sleep
            args:
                seconds: 30
            next: check_job

        - refresh_bq_external_tables:
            call: googleapis.run.v1.namespaces.jobs.run
            args:
                name: ${"namespaces/" + project_id + "/jobs/ch05-refresh-bq-iceberg-tables"}
                location: ${job_location}
            result: job_execution  

        - finish:
            return: 
                cloud_run_job_name: ${run_execution}
                dataproc_batch_id: ${dataproc_execution}